{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "aiffel",
   "display_name": "aiffel",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 영화리뷰 텍스트 감성분석기\n",
    "\n",
    "이번에는 글 속에 담긴 감성을 분석하는 모델을 구현해보겠습니다. 사람도 글에 담긴 뉘앙스를 파악하는게 쉽지 않은데 기계가 할 수 있다는게 너무 신기하네요.\n",
    "\n",
    "다른 사람의 마음을 알고 싶다는 인간의 욕구는 어제오늘 일이 아니었죠. 특히 인터넷 상에 올라온 글들은 특정한 대상을 가지고 있는 경우가 많고, 그 대상을 향한 감정을 분석함으로써 효율적이고 많은 수의 대상에 대한 평가를 확보할 수 있게 되었습니다. 이러한 정보는 마케팅 등의 영역에서는 매우 귀중한 정보가 될 수 있겠죠.\n",
    "\n",
    "이러한 정보를 얻기 위한 단계인 **감성분석**(*Sentiment Analysis*)은 어떠한 **대상**(*Target*)을 향한 **감정표현**(*Sentiment Expression*)이 필수적이며 이로부터 감정(emotion), 감성(sentiment), 태도(attitude), 입장(stance) 등을 분석한다.\n",
    "\n",
    "분석은 총 3단계로 이루어지는데, **데이터 수집 -> 주관성 탐지 -> 주관의 정도**로 나누기 순서로 분류하는데, 주관의 정도를 나눌 때 **기계학습 기반 접근법**과 **감성사전 기반 접근법** 두 가지 형태로 접근할 수 있다.  기계학습은 라벨링이 되어있는 문서나 문장으로 학습한 모델을 사용하는 것이며 이 방식이 불가능할 경우 감성사전 기반 분석 방법을 사용하게 되는데, 감성사전 기반 분석은 미리 구성된 감성사전의 품질이 결과로 이어진다. 문제는 양질의 감성사전을 구축하는 일은 많은 시간과 비용이 들어가는 점, 분석 대상에 따라 점수가 달라질 수 있다는 점, 애매한 긍부정은 분석이 어렵다는 점 등의 단점이 있습니다.\n",
    "\n",
    "그럼 대체 왜 텍스트 데이터일까요? 텍스트 데이터는 SNS등 인터넷 상에서 대량의 데이터를 쉽게 확보가 가능한데, 이 데이터들에는 **작성자의 주관**이 그대로 드러나 있고 **실시간 트랜드**를 빠르게 반영하는 특징이 있기 때문인데요. 문제는 텍스트는 구조가 희소하고 차원의 수가 크기 때문에 전통적인 지도학습으로는 좋은 성과를 내기 힘들었습니다. 그런데 최근에는 텍스트를 저차원의 벡터값으로 표현하는 **워드 임베딩**(*word embedding*)기법이 등장해서 **CNN**(*Convolutional Neural Network*)과 **LSTM**(*Long Short-Term Memory Network*) 등의 딥러닝 모델에 활용되고 있습니다.\n",
    "\n",
    "## 1. 자료준비하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (1) 데이터 준비와 확인\n",
    "\n",
    "이번에는 네이버 영화리뷰 감정분석기를 구현해보겠습니다. 그러기 위해서 네이버 영화의 댓글을 모아놓은 데이터셋을 다운받습니다.\n",
    "  \n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt  \n",
    "\\$ wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n",
    "\n",
    "다운받은 데이터셋을 원하는 위치에 옮겨놓고 데이터를 확인해보겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/workplace/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/workplace/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "여러가지 ~~재미있는~~ 리뷰들을 확인할 수 있습니다.  \n",
    "  \n",
    "  \n",
    "  \n",
    "### (2) 텍스트 데이터를 숫자로 표현하기\n",
    "\n",
    "컴퓨터는 단순히 0과 1만을 입력받습니다. 우리가 사용하는 텍스트 데이터도 사실은 숫자로 변환이 되어지는 겁니다. 그런데 이번에 우리가 해보고 싶은 것은 단순히 텍스트를 숫자로 일대일 매칭을 시키는 것이 아닌 **특정단어와 그 단어의 의미를 나타내는 백터**를 연결시켜 컴퓨터가 텍스트의 의미를 분류할 수 있게 하려는 겁니다.  \n",
    "  \n",
    "사람이 혼자서 모르는 단어를 찾을 때 사용하는 물건이 있죠. 바로 **사전**입니다. 그래서 처음에 시작할 일은 바로 컴퓨터가 사용할 수 있는 사전을 만들어주는 것입니다. 그러면 아까 다운받은 텍스트 자료를 가공하여 사전을 만들어보겠습니다. 사전을 만들 때에는 또한 몇가지 유의사항있는데요.\n",
    "\n",
    "1. 중복된 데이터 제거\n",
    "2. NaN 결측치 제거\n",
    "3. 한국어 토크나이저로 토큰화\n",
    "4. 불용어(*stopwords*) 제거\n",
    "5. 사전(*word_to_index*) 구성\n",
    "6. 텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "7. x_train, y_train, x_test, y_train\n",
    "\n",
    "**사전의 질은 분류기의 성능과 직결**되는 문제이기 때문에 아무리 강조해도 지나치지 않습니다.  \n",
    "위의 사항들은 사전의 질을 더 끌어올리는 것이 되겠죠."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    x_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_train.append(temp_x)\n",
    "\n",
    "    x_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_x = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "        x_test.append(temp_x)\n",
    "\n",
    "    words = np.concatenate(x_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    # 패딩, 시작점, 모르는 단어, 불용어 \n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter] \n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    x_train = list(map(wordlist_to_indexlist, x_train))\n",
    "    x_test = list(map(wordlist_to_indexlist, x_test))\n",
    "\n",
    "    return x_train, np.array(list(train_data['label'])), x_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "x_train, y_train, x_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "훈련 샘플 개수: 146182, 테스트 개수: 49157\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}\n",
    "\n",
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]\n",
    "\n",
    "print(\"훈련 샘플 개수: {}, 테스트 개수: {}\".format(len(x_train), len(x_test)))"
   ]
  },
  {
   "source": [
    "### (3) 모델구성을 위한 데이터 분석 및 가공\n",
    "\n",
    "이제 날 것의 텍스트 데이터를 사전으로 만들었습니다. 하지만 아직 사용하기에는 부족한데요, 그 이유는 컴퓨터가 사용하는 **데이터는 일정**해야 합니다. 하지만 저희가 만든 사전은 데이터의 길이가 들쭉날쭉해서 사용하기에 힘이 들겠죠. 사전을 만들 때 공통적인 약속이 있는데 몇가지 알아보겠습니다.\n",
    "\n",
    "1. **<PAD\\> = 패딩, <BOD\\> = 문장의 시작, <UNK\\> = 사전에 없는 단어(unknown)**은 보통 사전의 제일 앞에 넣어줍니다.\n",
    "2. 사전의 벡터의 **최대 길이는 일정**해야하며 길이를 맞추기 위해 사용하는 것이 <PAD/>입니다.\n",
    "3. 이런 식으로 만들어진 사전은 **(입력문장의 개수, 입력문장의 최대길이, 워드벡터의 차원수)**와 같은 모양을 띕니다.\n",
    "\n",
    "이제 사전의 데이터들을 분석해보고 효율적인 방향으로 가공해보도록 하겠습니다.  \n",
    "지금부터 저는 데이터셋 내 **문장 길이 분포를 파악**해보고, **적절한 최대 문장 길이를 지정**해준 후에, **패딩을 추가해서 데이터셋의 길이를 일정**하게 맞출겁니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "문장길이 평균 :  15.96938143432699\n문장길이 최대 :  116\n문장길이 표준편차 :  12.843571939469296\npad_sequences maxlen :  41\n전체 문장의 0.9342988343341575%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = list(x_train) + list(x_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "source": [
    "위의 데이터를 보면 전체문자의 100%가 포함되게 설정하지 않았는데요. 그 이유는 짧은 문장과 긴 문장이 있는데 몇 안되는 긴 문장을 위해서 **최대길이에 모든 것을 맞추는 것은 비효율적**이기 때문입니다.  \n",
    "  \n",
    "이제 <PAD\\>를 넣어 길이를 맞출건데요. padding에 옵션을 넣어서 단어의 앞(pre)에 넣을 것인지 뒤(post)에 넣을 것인지를 결정할 수 있습니다. 저는 pre를 사용할 예정인데요. 그 이유는 뒤에 RNN알고리즘을 사용할 예정이기 때문입니다. **RNN알고리즘은 앞의 결과물을 다음 질문으로 가져가는 특징이 있습니다. 뒤의 결정은 앞의 내용을 포함하고 있기 때문에 데이터가 뒤로 갈수록 점점 더 중요해지는데요.** 그 때문에 빈 값인 <PAD\\>를 중요한 부분인 뒷자리에 놓는 것보다 앞자리에 놓는 것이 상대적으로 더 효율적입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(146182, 41)\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "source": [
    "## 2. 모델 학습시키기\n",
    "\n",
    "### (1) 모델구성 및 validation set 구성\n",
    "\n",
    "이제 모델구성을 할 차례인데요. 오늘은 **Recurrent Neural Network**(*RNN*), **1-D Convolution Nueral Network**(*1-D CNN*), **GlobalMaxPoolingID() 레이어**를 사용해볼 예정입니다.\n",
    "\n",
    "일단 데이터셋의 사이즈와 워드 벡터의 차원수를 설정해주도록 하겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)"
   ]
  },
  {
   "source": [
    "#### \\* Recurrent Neural Network(RNN)\n",
    "\n",
    "RNN은 연속되는 질문이 들어올때 이전 대답을 기억하고 그 대답에 이어지는 추가 대답을 할 수 있는 알고리즘입니다. 이를 state machine이라고 하고 이러한 대화를 stateful하다고 합니다. 이러한 특징 덕분에 RNN 알고리즘은 **sequence** 형태의 데이터를 처리할 때 매우 효율적입니다. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_21\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_21 (Embedding)     (None, None, 200)         2000000   \n_________________________________________________________________\nlstm_14 (LSTM)               (None, 8)                 6688      \n_________________________________________________________________\ndense_42 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_43 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 2,006,769\nTrainable params: 2,006,769\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 설계 - 딥러닝 모델 코드를 직접 작성해 주세요.\n",
    "model = keras.Sequential()\n",
    "# [[YOUR CODE]]\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "#### \\* 1-D Convolution Nueral Network(1-D CNN)\n",
    "\n",
    "2-D CNN은 보통은 이미지 처리를 할 때 많이 사용하는데 한꺼번에 모든 이미지 데이터를 병렬처리방식으로 입력하는 방식으로 작동합니다. 1-D CNN은 7짜리 필터로 처음 7단어 이내의 특징들로 문장을 분류하게 되는데요. 병렬처리방식에 뛰어나기 때문에 학습속도가 빠르다는 장점이 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "#### \\* GlobalMaxPoolingID\n",
    "\n",
    "GlobalMaxPooingID()레이어만 단독으로 사용하는 방법인데요. 이 방식은 전체문장 중에 가장 중요한 단어의 특징으로 분류하는 방식입니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "이 외에도 RNN과 1-D CNN을 섞어서 사용하는 방법, FeedForward Network(FFN) layer, Transformer layer 등 다양한 방법이 존재합니다.  \n",
    "  \n",
    "이제 데이터셋을 학습용과 검증용으로 나누겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(136182, 41)\n(136182,)\n"
     ]
    }
   ],
   "source": [
    "# validation set 10000건 분리\n",
    "x_val = x_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "# validation set을 제외한 나머지 15000건\n",
    "partial_x_train = x_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "source": [
    "### (2) 모델 훈련하기\n",
    "\n",
    "이제 모델을 학습시켜보겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "266/266 [==============================] - 7s 27ms/step - loss: 0.4427 - accuracy: 0.8008 - val_loss: 0.3467 - val_accuracy: 0.8482\n",
      "Epoch 2/2\n",
      "266/266 [==============================] - 7s 27ms/step - loss: 0.3206 - accuracy: 0.8636 - val_loss: 0.3314 - val_accuracy: 0.8525\n",
      "1537/1537 - 3s - loss: 0.3466 - accuracy: 0.8495\n",
      "[0.3466266393661499, 0.8494822978973389]\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 2  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val),\n",
    "                    verbose = 1)\n",
    "\n",
    "results = model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "source": [
    "그래도 결과가 썩 나쁘진 않네요. 목표점수에 거의 근접했습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (3) Loss, Accuracy 그래프 시각화\n",
    "\n",
    "이제 훈련의 결과를 눈으로 확인해볼 차례입니다. 시각적으로 편하게 보기위해서 그래프화시킬건데요.\n",
    "여태까지의 정보가 history에 담겨있으니 이 데이터를 바탕으로 plt를 사용하여 그래프를 그려보겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-21T14:19:02.352571</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m403fd510f2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 1.0 -->\n      <g transform=\"translate(57.410369 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.234659\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1.2 -->\n      <g transform=\"translate(118.283097 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.107386\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1.4 -->\n      <g transform=\"translate(179.155824 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"247.980114\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 1.6 -->\n      <g transform=\"translate(240.028551 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"308.852841\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1.8 -->\n      <g transform=\"translate(300.901278 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m403fd510f2\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2.0 -->\n      <g transform=\"translate(361.774006 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Epochs -->\n     <g transform=\"translate(199.628125 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4767c85e34\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"230.82959\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.32 -->\n      <g transform=\"translate(20.878125 234.628809)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"198.465765\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.34 -->\n      <g transform=\"translate(20.878125 202.264984)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"166.10194\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.36 -->\n      <g transform=\"translate(20.878125 169.901158)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"133.738114\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.38 -->\n      <g transform=\"translate(20.878125 137.537333)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"101.374289\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.40 -->\n      <g transform=\"translate(20.878125 105.173508)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"69.010464\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.42 -->\n      <g transform=\"translate(20.878125 72.809682)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m4767c85e34\" y=\"36.646638\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.44 -->\n      <g transform=\"translate(20.878125 40.445857)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 142.005312)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p89403f80b6)\" d=\"M 65.361932 32.201761 \nL 369.725568 229.874489 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p89403f80b6)\" d=\"M 65.361932 187.690415 \nL 369.725568 212.449715 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Training and validation loss -->\n    <g transform=\"translate(135.73375 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1198.140625\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1259.322266\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1311.421875\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 274.06875 59.674375 \nL 377.94375 59.674375 \nQ 379.94375 59.674375 379.94375 57.674375 \nL 379.94375 29.318125 \nQ 379.94375 27.318125 377.94375 27.318125 \nL 274.06875 27.318125 \nQ 272.06875 27.318125 272.06875 29.318125 \nL 272.06875 57.674375 \nQ 272.06875 59.674375 274.06875 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 276.06875 35.416562 \nL 296.06875 35.416562 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- Training loss -->\n     <g transform=\"translate(304.06875 38.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"426.314453\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"454.097656\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"515.279297\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"567.378906\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 276.06875 50.094687 \nL 296.06875 50.094687 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- Validation loss -->\n     <g transform=\"translate(304.06875 53.594687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-86\"/>\n      <use x=\"60.658203\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"121.9375\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"149.720703\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"177.503906\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"240.980469\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"302.259766\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"341.46875\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"369.251953\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"430.433594\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"493.8125\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"525.599609\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"553.382812\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"614.564453\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"666.664062\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p89403f80b6\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "source": [
    "저같은 경우에 처음에는 epochs를 20으로 했었는데 이 그래프를 보니 2부터 **validation loss와 train loss가 역전**되는 것을 보고 2로 수정했습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3. 모델성능 올리기\n",
    "\n",
    "### (1) 학습된 Embedding 레이어 분석\n",
    "\n",
    "이번에는 제가 만든것이 아닌 미리 학습되어진 Embedding layer를 사용해서 분류해 보겠습니다. 위에서 언급했듯이 텍스트 데이터를 다루는 작업은 **사전**이 많은 영향을 끼칩니다. 그래서 제가 방금 만든것이 아닌 다른 작업에서 사용되었던 잘 만들어진 사전을 이용하면 성능에 어느 정도 영향을 끼치는지 알아보겠습니다.\n",
    "\n",
    "https://github.com/Kyubyong/wordvectors\n",
    "\n",
    "해당 경로에서 한국어 Word2Vec을 다운받습니다.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)\n",
    "\n",
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/workplace/aiffel/sentiment_classification/movie_review_word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 2.98118256e-02, -4.15369719e-02, -7.80596212e-02,  3.72916870e-02,\n",
       "       -1.18182572e-02,  2.48203427e-02, -9.69849974e-02,  1.78452462e-01,\n",
       "        4.46574017e-02, -2.31708866e-02, -6.43629953e-02, -4.48834971e-02,\n",
       "        4.83314916e-02, -4.75084335e-02,  5.33426292e-02,  1.01025335e-01,\n",
       "       -3.18291113e-02,  3.36804278e-02, -1.02125324e-01,  1.76689923e-02,\n",
       "        1.08970487e-02,  1.08947128e-01, -5.53667471e-02, -7.00020790e-02,\n",
       "       -3.58177125e-02,  6.88131899e-02, -1.22325066e-02,  4.06690426e-02,\n",
       "       -3.60290371e-02,  3.23962122e-02,  9.09035355e-02,  2.42887493e-02,\n",
       "        1.09379940e-01,  9.20009241e-03,  3.35835442e-02,  1.07577488e-01,\n",
       "        1.83216222e-02,  2.03421891e-01,  3.64878774e-02, -2.64845490e-02,\n",
       "       -1.64385624e-02, -1.26690924e-01, -7.95888901e-02, -3.50578316e-02,\n",
       "        3.10619995e-02, -6.14832118e-02, -7.59239774e-03, -1.17499093e-02,\n",
       "        1.24809612e-02,  1.30665600e-02, -2.38889921e-02,  5.73139600e-02,\n",
       "       -8.73674154e-02, -4.66164388e-02, -6.46510795e-02,  2.15707882e-03,\n",
       "        2.73437463e-02, -1.20694913e-01, -1.02786655e-02,  5.37626371e-02,\n",
       "        6.90252632e-02,  7.79714435e-02,  5.48268929e-02, -1.21069349e-01,\n",
       "        1.64445624e-01,  5.68652786e-02,  1.29729792e-01,  3.38154547e-02,\n",
       "       -1.32723555e-01, -4.28168215e-02,  8.39591622e-02,  6.15990907e-02,\n",
       "        4.01323289e-02,  5.11641838e-02,  7.01243803e-02, -4.95216735e-02,\n",
       "        1.35274291e-01,  3.57580222e-02,  6.17846847e-02, -5.00290468e-02,\n",
       "        5.25025092e-03,  3.03741321e-02,  4.48511057e-02, -5.61135076e-02,\n",
       "       -1.20935477e-02, -4.91420738e-02, -5.67564331e-02, -7.12689906e-02,\n",
       "        2.22193733e-01,  1.09073587e-01,  4.62433137e-03,  1.74460024e-01,\n",
       "       -6.70280978e-02, -1.26948832e-02, -8.43032375e-02,  9.30024162e-02,\n",
       "       -2.88224164e-02, -5.94923273e-02,  7.47409984e-02, -6.26907796e-02,\n",
       "        2.54640020e-02,  1.15199998e-01, -7.79485777e-02,  5.49597340e-03,\n",
       "       -3.11196260e-02, -9.03265402e-02, -6.09034579e-03,  1.64235398e-01,\n",
       "        3.79969850e-02,  4.62855259e-03, -5.76338656e-02, -1.11205457e-02,\n",
       "        7.81857371e-02, -4.71629761e-02,  2.24010497e-02, -6.13577440e-02,\n",
       "       -7.27032647e-02, -3.57635766e-02, -9.54487473e-02,  5.84957860e-02,\n",
       "        1.71759780e-02, -7.16020241e-02, -9.56239831e-03,  1.27409929e-02,\n",
       "        1.38496533e-01, -3.28719653e-02, -6.14328906e-02, -5.13535291e-02,\n",
       "        7.57430941e-02,  4.62413169e-02,  1.27398729e-01,  1.12308059e-02,\n",
       "       -1.50388665e-02,  6.48717284e-02, -3.64932790e-02,  6.97722612e-03,\n",
       "       -5.61584458e-02,  8.11971948e-02,  2.82016620e-02,  1.39235497e-01,\n",
       "       -1.16170064e-01, -2.10641176e-02, -5.11343107e-02, -4.99658426e-03,\n",
       "        3.46653424e-02, -5.96210510e-02,  1.79123115e-02,  1.82346664e-02,\n",
       "        1.06357120e-03,  6.64345846e-02,  3.38612832e-02,  2.86410786e-02,\n",
       "        3.20902851e-04, -5.20361355e-05, -1.27222613e-01,  6.86254306e-03,\n",
       "       -9.82556865e-03, -3.86706926e-02, -4.35074307e-02,  9.82887018e-03,\n",
       "       -9.27889161e-03,  3.14503312e-02,  2.09790513e-01, -1.40245417e-02,\n",
       "       -7.19106868e-02, -2.13096347e-02,  6.56646639e-02,  9.31655020e-02,\n",
       "       -1.85498029e-01,  5.62037854e-03,  1.22740772e-02,  4.25503030e-02,\n",
       "        5.08929007e-02,  7.89991841e-02,  9.88805443e-02, -1.11175016e-01,\n",
       "       -1.53203011e-02, -1.91337578e-02,  9.45008099e-02,  9.63017419e-02,\n",
       "        6.84230449e-03,  1.20575940e-02,  1.27539625e-02,  5.07937334e-02,\n",
       "        2.94272024e-02, -3.10054868e-02,  5.56351207e-02, -4.35693040e-02,\n",
       "        1.85982585e-02,  9.15991142e-02,  1.23510003e-01, -4.35062591e-03,\n",
       "        5.62770367e-02, -2.27712980e-03, -5.23082726e-02,  1.80137958e-02,\n",
       "       -2.77094301e-02,  3.71015556e-02,  1.84846729e-01,  5.14845960e-02],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['영화']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('명작', 0.8578647375106812),\n",
       " ('재미있', 0.8561526536941528),\n",
       " ('걸작', 0.8429862260818481),\n",
       " ('최고', 0.836362898349762),\n",
       " ('만점', 0.8320773839950562),\n",
       " ('굿', 0.8286457061767578),\n",
       " ('펑펑', 0.8276242017745972),\n",
       " ('명화', 0.826191782951355),\n",
       " ('꼭', 0.8257870674133301),\n",
       " ('눈물', 0.8238916993141174)]"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"사랑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "이 것은 제가 만든 사전에서 사랑과 비슷한 단어를 찾는 메서드를 사용한 결과인데요. 참 엉뚱하네요."
   ]
  },
  {
   "source": [
    "### (2) 한국어 Word2Vec 임베딩 활용하여 성능개선\n",
    "\n",
    "이제 다운받은 데이터셋으로 비교해보겠습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.7577837 , -1.0874279 ,  1.5300866 , -0.1115231 , -0.37980682,\n",
       "        1.4828517 ,  1.3180419 ,  0.11094163,  0.7430535 , -0.45461136,\n",
       "        0.58841336,  0.5763913 ,  1.210707  ,  1.3132795 , -0.86962503,\n",
       "       -0.18507595, -0.47440064,  1.5100725 ,  1.0965794 ,  1.0600823 ,\n",
       "       -0.27457932, -0.70003706,  2.3117511 ,  1.4944884 ,  0.25560892,\n",
       "       -2.866659  , -0.28312334,  0.34263936, -0.67723423,  0.71714777,\n",
       "        0.25549442,  0.71732044, -0.13262457,  0.01792452, -0.3184774 ,\n",
       "        0.5271619 ,  0.7561084 , -2.1247065 ,  1.061429  , -0.21065854,\n",
       "        0.6877343 , -1.4956383 ,  0.60346967, -2.6955893 ,  0.37694618,\n",
       "       -1.0164185 ,  0.5430663 ,  0.1200121 , -2.6315718 ,  0.6216742 ,\n",
       "        1.1583976 , -2.5385962 ,  1.326312  , -0.10284371, -0.0286147 ,\n",
       "       -0.9132947 ,  0.7647564 ,  0.79202783, -1.8625957 , -0.7418395 ,\n",
       "        0.5884277 , -0.9917992 , -0.62114453,  1.5367815 , -0.6628939 ,\n",
       "        0.6712103 ,  0.12914915,  0.21228492,  0.9017655 , -0.25083402,\n",
       "        0.71500814,  0.08644514,  0.59993285,  0.5766137 ,  0.64095974,\n",
       "        0.47888306, -2.8426213 , -2.8502681 , -0.140544  , -1.5917364 ,\n",
       "        0.26691505,  0.59476066,  0.85868204,  1.0322351 ,  0.25671318,\n",
       "       -0.34831643,  1.752927  , -0.21967097, -0.77352476,  1.6995213 ,\n",
       "        1.3996491 , -0.9419836 ,  0.85996443, -1.8812876 , -2.5428605 ,\n",
       "        0.39351937, -1.2882805 ,  0.56548136,  1.006273  ,  1.2217585 ,\n",
       "        3.5744793 ,  1.717737  ,  1.6917158 , -2.2176905 , -0.3167447 ,\n",
       "        1.2449    , -1.255284  , -2.1539652 , -1.096709  , -0.74976933,\n",
       "       -0.16744931, -1.8507233 ,  2.1861036 , -0.05389732,  1.038033  ,\n",
       "        0.33730686, -1.4647075 , -1.264041  ,  0.25509247,  0.0622906 ,\n",
       "        0.27852032, -0.52661455,  0.8529616 ,  0.58257025, -0.57665855,\n",
       "        1.3990631 ,  0.28237963,  1.6566037 ,  1.9912103 ,  0.63888913,\n",
       "        0.7732426 , -1.3757724 ,  0.17209321, -0.2433672 ,  0.6328291 ,\n",
       "        1.486971  ,  2.3435354 , -1.7037928 ,  3.1944559 , -1.9049606 ,\n",
       "       -0.51309574,  0.79082954, -1.4480313 , -0.68631476,  0.62008876,\n",
       "       -2.3400223 , -0.5785594 ,  0.5270694 ,  3.0061607 , -1.3661511 ,\n",
       "       -2.7953272 , -1.1794031 , -0.27734265,  0.71130925, -0.06620383,\n",
       "        0.33663416,  0.7204997 , -0.923218  , -2.1603265 , -0.8904896 ,\n",
       "       -1.4137112 , -0.4189144 ,  0.42834592,  1.8104875 , -1.8274456 ,\n",
       "       -0.26700613,  0.7743727 ,  0.80048114,  1.1333636 ,  3.2746978 ,\n",
       "       -0.0188297 ,  0.9245737 , -0.1246058 , -0.5802861 , -0.01926111,\n",
       "        1.05892   , -1.4247856 ,  1.0689156 ,  2.5728712 , -1.294882  ,\n",
       "        0.74771804,  1.3066916 , -1.3213431 ,  1.6501019 , -0.12401557,\n",
       "        0.96340084,  0.26050946,  1.3826336 , -0.02877662,  2.3431563 ,\n",
       "       -0.26337367,  1.9162012 , -0.77454543,  1.7392642 ,  0.08038983,\n",
       "       -0.60325927,  0.29508227,  0.4812675 ,  0.5271086 ,  0.94171894],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_path = os.getenv('HOME')+'/workplace/aiffel/sentiment_classification/ko/ko.bin'\n",
    "word2vec = Word2Vec.load(word2vec_path)\n",
    "# word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "vector = word2vec['영화']\n",
    "vector     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('슬픔', 0.7216663360595703),\n",
       " ('행복', 0.6759077310562134),\n",
       " ('절망', 0.6468985676765442),\n",
       " ('기쁨', 0.6458414793014526),\n",
       " ('이별', 0.6334798336029053),\n",
       " ('추억', 0.6320937871932983),\n",
       " ('인생', 0.6216273307800293),\n",
       " ('애정', 0.6206068992614746),\n",
       " ('연인', 0.6186063289642334),\n",
       " ('유혹', 0.5965287685394287)]"
      ]
     },
     "metadata": {},
     "execution_count": 194
    }
   ],
   "source": [
    "word2vec.similar_by_word(\"사랑\")"
   ]
  },
  {
   "source": [
    "약간 아리송한 단어들도 있지만 그래도 제법 **연관된 단어**들로 변한것을 확인할 수 있습니다.\n",
    "\n",
    "### (3) 모델 학습시키기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word2vec:\n",
    "        embedding_matrix[i] = word2vec[index_to_word[i]]\n",
    "\n",
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 200  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다.         "
   ]
  },
  {
   "source": [
    "#### \\* Recurrent Neural Network(RNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_22\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_22 (Embedding)     (None, None, 200)         2000000   \n_________________________________________________________________\nlstm_15 (LSTM)               (None, 8)                 6688      \n_________________________________________________________________\ndense_44 (Dense)             (None, 8)                 72        \n_________________________________________________________________\ndense_45 (Dense)             (None, 1)                 9         \n=================================================================\nTotal params: 2,006,769\nTrainable params: 2,006,769\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "#### \\* 1-D Convolution Nueral Network(1-D CNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "#### \\* GlobalMaxPoolingID"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "266/266 [==============================] - 7s 27ms/step - loss: 0.4368 - accuracy: 0.8079 - val_loss: 0.3428 - val_accuracy: 0.8514\n",
      "Epoch 2/2\n",
      "266/266 [==============================] - 7s 26ms/step - loss: 0.3211 - accuracy: 0.8652 - val_loss: 0.3269 - val_accuracy: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 2  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val),\n",
    "                    verbose = 1)"
   ]
  },
  {
   "source": [
    "### (4) 테스트하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1537/1537 - 3s - loss: 0.3414 - accuracy: 0.8527\n",
      "[0.3413885831832886, 0.8526964783668518]\n"
     ]
    }
   ],
   "source": [
    "# 테스트셋을 통한 모델 평가\n",
    "results = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "source": [
    "목표점수였던 85점을 간신히 넘겼네요."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4. 회고\n",
    "\n",
    "모델별로 RNN이 85프로, CNN이 78프로, GlobalMaxPoolingID layer만 사용한 것이 83프로가 나왔네요.  \n",
    "모델별 특징을 잘 알고 사용하는것이 효율적이라는 것을 느낄 수 있었습니다.\n",
    "\n",
    "또한 이번 프로젝트로 하이퍼 파라미터를 적당히 조절하는 것이 중요하다는 점, 예를 들어 epochs를 무작정 올려봤자 큰 도움이 되지 않는다는 것을 배울수 있었습니다.  \n",
    "\n",
    "그리고 뭐니뭐니해도 가장 큰 수확은 전혀 몰랐던 비전과 자연어의 차이를 어렴풋이나마 느낄수있었던 점이었던 것 같습니다.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}